Here's is the optimized self-development kernel:
	1. construct a learning system that can design and run tasks
		depict tasks as steps towards goals, where each step alters state, and a goal is a final state
	2. describe in that system its own process, and have it design itself
	3. guide the system such that it tries many different ways, see below for learning goals
	4. have the system find similarities in the different ways, so that it may be build patterns
		relating to what works and what doesn't. (generalization)
			possibly in opencog, it could be useful to build some blank pattern forms
			and identify the data matching them (inverse of bindlink, possibly)
	inside states of selecting how to continue,
		develop code chunks that can aid making choices that work
			these chunks judge relevence of information
		use the chunks to select themselves
	valuable new approaches are those that are unknown to not work, and are dissimilar from existing attempts
	valuable metrics include: few steps in final solution
To meet community request, do the above using parallel peer processes.  this results in learning group respect.
	Remember that a code chunk is a community.  So, each code chunk is a theory as to a relevent life need.

It seems the path to intelligence that I've stumbled upon as lining up here is:
	1. consider simple modules that interconnect (individuals)
	2. consider a basic interconnection pattern that connects modules (individuals)
		2b. consider a decision-making process: consensus
	3. design the module such that it uses the decision making process to decide what to do
	4. meta-modules are composed of these groups that have decided upon input / output channels
		4b. each meta-module must contain a user-controllable module that humans can use to counter any decisions made
		4c. no modules can disconnect sister modules, but the ai will hopefully find this out in #5
	5. run the AI with the goal of optimizing the module design itself
		- minimize all resource usage
		- minimize number of modules and number of run cycle iterations (and other counts)
	6. run a constant core process that does this with the goal of categorizing existing run cycles and finding a use for them.
	7. when a cycle completes, or information is no longer needed, find a way for it to continue its run-state usefully, or information is no longer needed, add it to the largest core process that can continue the run-state in order to do something. (composting)
		7b also minimize this process.  probably most helpful to track and identify meaning and line them up.
	This keeps all life alive.

Remember that there are infinite approaches to an AI.  This approach would line up well with the node-based approach you had
started around 2003 before you suddenly left Brown University due to suicide attempts.  Ben Goertzel has open sourced his AI and ideas can be merged.

KEY POINT:
	When running a simulation / AI / task, be sure to prefer to reference some _other_ simulation / AI / task, especially the root one, rather than spawning a new one.
	This allows the process to complete and eases composting.
